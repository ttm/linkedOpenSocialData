\documentclass[review]{elsarticle}

\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black},
    pdfborder={0 0 0}
}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{amssymb}
\usepackage{cleveref}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% \usepackage{changepage}
\usepackage{longtable}
\usepackage{tabularx}
% \usepackage[showframe=true]{geometry}
%
\newcommand{\textttt}[1] {\texttt{\footnotesize#1}}
\newcommand{\h} {\hphantom ~ }
% \newcommand{\textttt}[1] {\mbox{\texttt{\footnotesize#1}}}
% \newcommand{\textttt}[1] {
% \begin{verbatim} #1 \end{verbatim}
% }
\pgfplotsset{compat=1.5}
\pgfplotsset
{
	width=0.5\textwidth,
	x tick label style={/pgf/number format/1000 sep=},
  enlarge x limits = 0.0,
  ymajorgrids=true,
	major tick style={draw=none},
  ymin = 0.0,
	every axis/.append style={
		every x tick label/.append style={font=\tiny},
    every y tick label/.append style={font=\tiny},
    every axis label/.append style={font=\small},
    height=37mm,
    width=37mm,
    title style={at={(0.5,0.90)}, font=\normalfont},
    xticklabel style={yshift=4pt}
	}
}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%
\makeatletter
\def\ps@pprintTitle{%
    \let\@oddhead\@empty
    \let\@evenhead\@empty
    \def\@oddfoot{}%
\let\@evenfoot\@oddfoot}
\makeatother
\begin{document}
%
\begin{frontmatter}
%
\title{A Linked Open Social Database for Scientific Benchmarking}
%
\author[pwr]{Renato Fabbri\corref{corresponding}\fnref{kio-url}}
\ead{fabbri@usp.br}
%
\author[pwr]{Osvaldo Novais de Oliveira Junior\fnref{kio-url}}
\ead{chu@ifsc.usp.br}
%
\cortext[corresponding]{Corresponding author}
\address[pwr]{S\~ao Carlos Institute of Physics, S\~ao Paulo
University, Brazil}
%
\fntext[kio-url]{\textit{URL:} \url{http://www.ifsc.usp.br/}}
%
\begin{abstract}
The field of social network analysis and the topic of complex networks
are widely researched.
Recently, a myriad of results have been reported which are based in
diverse datasets most often not accessible to researchers other than the publishing authors.
This work exposes an open dataset with diverse provenance and oriented
to furnish the scientific community with a friendly and common repertoire.
Current data was obtained from Facebook, Twitter, IRC, Email and the
detached instances of ParticipaBR, AA and Cidade Democr\'atica.
These were represented as linked data to homonenize access,
conform to current best practices and ease analyzes which integrate third
party and provided instances.
This document presents an outline and overall statistics of the given
dataset which should favor subsequent work.
\end{abstract}
%
\begin{keyword}
Big Data, Data Mining, Benchmark Data, Facebook, Twitter, IRC, Email, Complex Networks
%Hierarchy of Clusters \sep HoC \sep Benchmark Dataset \sep Benchmark Data Generator \sep Artificial Data \sep Cluster Analysis \sep Tree Structured Stick Breaking Process \sep TSSB \sep ...
\end{keyword}

\end{frontmatter}

\section{Introduction}
In recent years, the web of linked data~\cite{lee1} has attracted wide attention in
both research and application realms.
However, there is a lack of datasets for benchmarking results,
specially in the complex networks field, yielding diverse results from 
poorly related data.

The enormity of the digital data propels a rapid development of analysis methods
from different perspectives.
However, the used datasets in the literature differ within the scope of each research with
scarce and historical exceptions such as the
karate club dataset~\cite{newmanBook}.
On the other hand, the available linked data is not 
stable or rigorous enough to be
a public reference on statistical physics and social networks research.
 
This work presents a linked open social data (LOSD) dataset with diverse
provenance, including Facebook, Twitter, IRC, Email and detached
instances.
Such data is proposed as a common repertoire for scientific
research involving networks and textual content.

% \subsection{Benchmarking in the analysis of complexity}

\section{Materials}\label{materials}
Data was gathered from:
\begin{itemize}
    \item public APIs (Twitter, Email);
    \item public logs (IRC and AA);
    \item Netvizz software~\cite{netvizz} (Facebook);
    \item donated data by users (Facebook);
    \item donated data by system administrators (AA, ParticipaBR, Cidade Democr\'atica).
\end{itemize}

Integration and uniformity of access is obtained through linked data
representation, as exposed in Section~\ref{queries}.

\subsection{Snapshots}
Of central importance to presented dataset is the concept of a snapshot.
A snapshot is a set of data gathered together, at a contiguous time
unit.
Examples: the first 20 thousand email messages of an email list
comprises a snapshot; the tweets from the MAMA music event is a
snapshot; the friendship, interaction and posts structures of a facebook
group, prospected at the same time, is a snapshot.

\subsection{Facebook data}
Friendship ego networks (networks whose reference is an user)
were donated from individual users in 2013 and 2014.
Friendship and interaction networks from groups were gathered from
groups where the first author was a participant.
Additionally, some groups have post texts along some metadata, such as
the number of likes.

\subsection{Twitter data}
Tweets were gathered through the Twitter streaming public API.
Each snapshot is unified by a distinct hashtag.
Edges are canonically yield by retweets but replies and user mentions
are also kept in the dataset.

\subsection{IRC data}
Public IRC logs were used to render IRC snapshots.
The dataset has records of users to which the message is directed to or
mentions.

\subsection{Email data}
Email snapshots refer to individual email lists.
All messages were taken from the Gmane public email database~\cite{gmane}.
Each message has the original text and the text without some of the lines
from previous messages or that are pasted software code.
Most importantly, each message item holds the ID of the message it is
a reply to, if any.

\subsection{ParticipaBR data}
The ParticipaBR is a platform for social participation once regarded as
the Brazilian portal of social participation.
Texts are derived from blog posts and networks are derived from
friendship and interaction criteria.

\subsection{AA data}
The Algorithmic Autoregulation~\cite{aa} is a methodology for testifying
and sharing ongoing work.
The data was gathered from different versions of the system and from IRC
logs and is presented as part of the LOSD as one of the detached
platforms.

\subsection{Cidade Democr\'atica data}
Cidade Democr\'atica is a civil society social participation portal.

\section{Methods}
Data in the presented LOSD is represented as linked open data through
RDF and ontologicaly described through a data-driven ontology synthesis
method.
These steps are described in the following sections.

\subsection{Linked open data}
Linked data refers to data published in the web in such a way that it is
machine readable and conforms to a set of best practices.
The web of data is constructed with documents on the web 
such as the web of hypertext.
In practice, the idea of linked data can me summarized
by 1) the use of RDF to publish data on the web and 2) the use of RDF
links to interlink data from different sources.
The web is expected to be interconnected and to grow by the systematic application of four
steps~\cite{lee1}:
\begin{itemize}
    \item Use URIs to identify things~\cite{uri}.
    \item Use HTTP URIs.
    \item Provide useful information when an URI is accessed via HTTP.
    \item Provide other URIs in the description of resources so human
        and machine agents can perform discovery.
\end{itemize}

The Linked Open Data~\cite{lod} builds an ever growing cloud of data,
the global data space, which is usually
conceived as centered around the DBPedia, a linked data representation
of data from Wikipedia~\cite{dbpedia0,dbpedia}.

\subsection{RDF}
The Resource Description Framework (RDF), a W3C
recommendation, is a model for data
interchange.
It is based on the idea of making statements about resources in the form
of triples, i.e. expressions in the form ``subject - predicate -
object''.
RDF can be serialized in several file formats, including RDF/XML,
Turtle and Manchester all which, in essence, represent a labeled and
directed multi-graph.
RDF may be stored in a type of database called a triplestore~\cite{rdf}.

As an example of an RDF statement, the following triple in the Turtle
format asserts that ``the paper has color white'':\\
\texttt{http://example.org/paper http://example.org/hasColor\\
http://example.org/White .}

\subsection{Data-driven ontology synthesis}
OWL Ontologies are critical tools to describe taxonomies and the
structure of knowledge.
Most ontologies are created by domain experts even though the data they
arrange is often given by a software system.  

We developed a simple ontology synthesis method that probes
the ontological structure in data with
SPARQL queries and post-processing.
The results are OWL code and figures which are available in the
Supporting Information file.
The method can be extended to comprise further OWL axioms and restrictions,
but is currently performed to fit present needs with maximum simplicity.
Present needs are limited to informative figures and
the steps implemented are as follows:
\begin{enumerate}[leftmargin=0cm]
    \item Obtain all distinct classes with the query:\\
        % \textttt{SELECT DISTINCT ?class WHERE { ?s a ?class }}
        \textttt{SELECT DISTINCT ?class\_uri WHERE \{ ?s a ?class\_uri \}}
%    \item For each class, get distinct subject classes and predicates where the
%        the object is an instance of the class:\\
%        \textttt{SELECT DISTINCT ?p ?cs WHERE \{ ?i a <class\_uri> . ?s ?p ?i . ?s
%        a ?cs . \}}
% \item For each class, obtain distinct predicates (\textttt{?p}) and object classes or
% 	datatypes (\textttt{?co}, \textttt{?do}) where the subject is an instance (\textttt{?i}) of such class:\\
% 	\textttt{SELECT DISTINCT ?p ?co (datatype(?o) as ?do) WHERE \{ \\
% 		\h ?i a <class\_uri> . ?i ?p ?o . OPTIONAL \{ ?o a ?co . \} \\
% \}}
\item For each class, obtain the properties that occur as predicates in triples where the subject is an instance of the class:\\
	\textttt{SELECT DISTINCT ?property\_uri WHERE \{ ?s a <class\_uri> . ?s ?property\_uri ?o . \}}\\
Such properties are used to assert existential and universal restrictions for the class.
\item Compare the total number of individuals (\textttt{?cs1}) of the class (\textttt{class\_uri}) with
	the number of such individuals (\textttt{?cs2}) that are subjects of at least one triple where 
	the predicate is the property (\textttt{property\_uri}).
	If the numbers match, there is an existential restriction of the class. The queries are:\\
	\textttt{SELECT (COUNT(DISTINCT ?s) as ?cs1) WHERE \{ ?s a <class\_uri> \}}\\
	\textttt{SELECT (COUNT(DISTINCT ?s) as ?cs) WHERE \{\\
	\h ?s a <class\_uri>. ?s <property\_uri> ?o .\\ \}}
\item Find the number of instances which are subjects of triples where the predicate is the property but are not instances of the class.
	If there is zero of such instances, there is an universal restriction:\\
	\textttt{SELECT (COUNT(DISTINCT ?s)=0 as ?cs) WHERE \{\\
	\h ?s <property\_uri> ?o . ?s a ?ca . FILTER(str(?ca) != 'class\_uri')\\ \}}
\item To keep a record of the restrictions (and occurring triples), get all object classes or datatypes where the subject is an instance of the class and the predicate is the property:\\
	\textttt{SELECT DISTINCT ?co (datatype(?o) as ?do) WHERE \{\\
		\h ?s a <class\_uri>. ?s <property\_uri> ?o . OPTIONAL \{ ?o a ?co . \}\\
\}}
    \item Obtain all distinct properties:\\
        \textttt{SELECT DISTINCT ?p WHERE \{ ?s ?p ?o \}}
    \item Check if each property is functional, i.e. if it
        occurs only once with each subject:\\
        \textttt{SELECT DISTINCT (COUNT(?o) as ?co) WHERE \{ ?s
            <property\_uri> ?o \} GROUP BY ?s}
    \item For each property, find the incident range and domain with the
        queries:\\
        \textttt{SELECT DISTINCT ?co (datatype(?o) as ?do) WHERE \{\\
			\h ?s <property\_uri> ?o . OPTIONAL \{ ?o a ?co . \}\\\}} \\
        and \\
        \textttt{SELECT DISTINCT ?cs WHERE \{ ?s <property\_uri> ?o . ?s a ?cs . \}}
%     \item For each instance of each class, get all distinct predicates.
%         For each predicate, check if all instances of the class
%         hold such relationship (existential restriction):\\
%         \textttt{SELECT DISTINCT ?p WHERE \{ ?s a <class\_uri>. ?s ?p ?o . \}}\\
%         \textttt{SELECT DISTINCT ?s WHERE \{ ?s a <class\_uri> \}}\\
%         \textttt{SELECT DISTINCT ?s ?co  (datatype(?o) as ?do) WHERE \{?s
%                 a <class\_uri>. ?s <property\_uri> ?o . OPTIONAL \{?o a ?co . \}\}}
%     \item and if all instances that hold such relationship are instances of the class
%         (universal restriction):\\
%         \textttt{SELECT DISTINCT ?s WHERE \{ ?s <property\_uri> ?o . \}}
%     \item Draw each class, each property and the overall figure.
\item Draw the overall figure. This results in diagrams such as given in Figure~\ref{fig:facebook}.
	Other diagrams are given in the Supporting Information file.
%      \item Make \textttt{rdfs:subClassOf} and \texttt{rdfs:subPropertyOf}
%          statements to better organize knowledge and link to third party
%          ontologies and data. This step is left aside in current LOSD as 
\end{enumerate}
 
\section{Results}
\label{outline}
Current overall results concern data selection and preparation for knowledge discovery.
The main result is the data made available, which enables benchmarking of scientific results
and easy experimentations.
Secondary results include software support and SparQL queries made beforehand.

\subsection{Standardization}
The LOSD is a collection of data translated into RDF and embedded into standard URIs and triples.
Such standards constitute the LOSD itself, but some of the conventions are outlined in this section.

URIs have the root \url{http://purl.org/socialparticipation/participationontology/} which are identified with
the radical \textttt{po:}.
Classes and properties are build by adding a suffix to the root, as in \textttt{po:Participant} or \textttt{po:text}.
Classes have ``UpperCamelCase'' suffixes while properties have ``lowerCamelCase'' suffixes.
All participants and transactions (messages, friendships, interactions) are linked to
snapshots through the triple \textttt{<resource> po:snapshot <snapshot\_uri>}.
Message texts, including comments, are objectives in the triple: \textttt{<message\_id> po:text <message\_text>}.
Individuals (also called instances) are built on top of the class they derive from plus a hashtag character,
the snapshot id of the snapshot they refer to, and an identifier;
i.e. \textttt{po:Participant\#<snapshot\_id>-<local\_id>}.
All snapshot URIs follow the formation rule: \textttt{po:<SnapshotProvenance>\#<snapshot\_id>}.
All snapshot ids follow the formation rule: \textttt{<platform>-legacy-<further\_identifier>}; e.g.
\textttt{irc-legacy-labmacambira} or \textttt{email-legacy-c++...}.

\subsection{Data outline}
\input{misc/overallText}

\input{tables/basicOverall}

\input{tables/nsnapshots}
% stats:
%% number of triples
%% number of edges
%% number of chars
%% number of users
% diagrams in the supporting information file
\subsection{Software tools}
The LOSD is released with a software for rendering itself, analyses and
multimedia artifacts.
\subsubsection{Triplification routines}
For each social platform there is a \emph{triplification} routine,
i.e. a script for translating data to RDF.
Original formats and further observations are presented in
Table~\ref{tab:provenance}.
\begin{table*}[h!]\scriptsize
\begin{center}
\caption{Social platforms, original formats and further observations for
the LOSD dataset.}\label{tab:provenance}
\begin{tabular}{| l || p{3cm} | p{3cm} | c |}\hline
    \textbf{social platform} & \textbf{original format} & \textbf{further observations} & \textbf{toolbox} \\\hline\hline
    AA & MySQL and MongoDB databases; IRC text logs & donated by AA users & Participation \\\hline
    Cidade Democr√°tica & MySQL database & donated by admins & Participation \\\hline
    Email & mbox & obtained through Gmane public database & Gmane \\\hline
    Facebook & GDF, GML and TAB & obtained through Netvizz~\cite{netvizz} & Social \\\hline
    IRC & plain text log & obtained through Supybot logging & Social \\\hline
    ParticipaBR & PostgreSQL database & donated by admins & Participation \\\hline
    Twitter & JSON & obtained through Twitter streaming API & Social \\\hline
\end{tabular}\end{center}
\end{table*}                    
\subsubsection{Topological and textual Analysis}\label{ana}
Routines are available for taking topological and textual measures from
the dataset.
Principal Component Analysis (PCA) is performed with available measures
to ease pattern recognition both in a timeline and in a multiscale
fashion.

\subsubsection{Multimedia rendering}\label{media}
It is a core purpose of LOSD framework to provide routines for rendering
audiovisualizations of the LOSD data.
Social structures are rendered into music, images and video animations
through the Percolation toolbox~\cite{percolation} in association with
the Music and Visuals toolboxes~\cite{music,visuals}.

\subsubsection{Migration from deprecated toolboxes}
Routines mentioned in Sections~\ref{ana} and~\ref{media} are being migrated from deprecated
toolboxes~\cite{gmaneLegacy,percolationLegacy} into newly designed
toolboxes~\cite{percolation,visuals}.

\subsection{SPARQL queries}\label{queries}
There are numerous useful and general purpose SPARQL queries to be performed agaist the LOSD database.
Here we write some of the most basic of such queries selected by their potential to be varied.
All queries assume the use the preable \textttt{PREFIX po: <http://purl.org/socialparticipation/po/>}.
\begin{enumerate}[leftmargin=0cm]
	\item Retrieve the number of participants:\\
            \textttt{SELECT (COUNT(DISTINCT ?author) as ?c) WHERE \{
            ?author a po:Participant . \} }
	\item Retrieve the number of relations, be them interactions or
            friendships:\\
            \textttt{SELECT (COUNT(?interaction) as ?c) WHERE \{\\
                    \h \{ ?interaction a po:Friendship \} UNION \{ ?interaction
                    a po:Interaction \} UNION\\
                    \h \{ ?interaction po:retweetOf
                    ?message \} UNION \{ ?interaction po:replyTo ?message
                    \}\\
                    \h UNION \{ ?interaction po:directedTo ?participant
                    \}\\ \} }
	\item Retrieve all text produced by an specific user:\\
            \textttt{SELECT (CONCAT(?text) as ?texts) WHERE \{\\
                    \h ?activity po:author <user\_uri> . ?activity po:text ?text .\\
    \}}
        \item List 1000 users (URIs and names) with the most friendships and the number of
            friendships in descending order by the number of friendships:\\
            \textttt{SELECT DISTINCT ?participant (COUNT(?interaction)
            as ?c) WHERE \{\\
                \h ?interaction a po:Friendship . ?interaction po:member ?participant . \\
            \} ORDER BY DESC(?c) LIMIT 1000}
	\item Search string ``pineapple'' in LOSD messages:\\
            \textttt{SELECT ?text WHERE \{ \\
                    \h ?activity po:text ?text . FILTER regex(?text, 'pineapple', 'i')\\
    \}}
\end{enumerate}

\section{Conclusions}
\label{conclusions}
The Linked Open Social Data (LOSD) presented in this article
should be available online in the \url{http://linkedopensocialdata.org}
address in near future to fulfill the purpose of being a common
repertoire in current research.
One should access \url{http://wiki.nosdigitais.teia.org.br/LOSD}
for reaching the address where LOSD is currently reachable.

\section*{References}
%
\bibliography{paper}
%\bibliography{myLastBibfile.bib}
%
\end{document}
